#!/usr/bin/env python3
"""Daily Research Workflow for HayekNet Graduate Project.

This script runs the complete daily workflow:
1. Fetch latest ERCOT data
2. Run HayekNet system (all 8 components)
3. Generate research notes and observations
4. Save results for thesis/paper

Usage:
    python scripts/daily_research_workflow.py
    python scripts/daily_research_workflow.py --no-data-fetch  # Skip data fetch
    python scripts/daily_research_workflow.py --quick          # Quick test run
"""
from __future__ import annotations

import sys
import argparse
from datetime import datetime, timedelta
from pathlib import Path
import json

import numpy as np
import pandas as pd

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

from python.data import ERCOTDataClient
from python.battery_daily_analysis import run_battery_daily_analysis
from python.research_observations import ResearchObservationTracker


def setup_directories():
    """Ensure research directories exist."""
    base_dir = Path(__file__).resolve().parents[1]
    
    dirs = {
        'archive': base_dir / 'data' / 'archive' / 'ercot_lmp',
        'reports': base_dir / 'data' / 'reports',
        'logs': base_dir / 'data' / 'logs',
        'research': base_dir / 'research',
        'journal': base_dir / 'research' / 'journal',
        'observations': base_dir / 'research' / 'observations',
        'results': base_dir / 'research' / 'results'
    }
    
    for dir_path in dirs.values():
        dir_path.mkdir(parents=True, exist_ok=True)
    
    return dirs


def fetch_daily_data(client: ERCOTDataClient, quick: bool = False) -> pd.DataFrame:
    """Fetch latest ERCOT data."""
    print(f"\n{'='*80}")
    print(f"STEP 1: Data Ingestion - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}\n")
    
    max_reports = 12 if quick else 100  # 1 hour vs ~8 hours
    
    print(f"üì• Fetching latest ERCOT LMP data...")
    print(f"   Mode: {'Quick test (1 hour)' if quick else 'Full (8 hours)'}")
    print(f"   Reports: ~{max_reports}")
    
    df = client.fetch_historical_data(
        report_type="real_time_lmp",
        max_reports=max_reports,
        use_cache=True
    )
    
    if df.empty:
        print("‚ö†Ô∏è  No new data available")
        return df
    
    # Parse timestamp
    if 'SCEDTimestamp' in df.columns:
        df['timestamp'] = pd.to_datetime(df['SCEDTimestamp'])
    
    print(f"\n‚úÖ Data ingestion complete:")
    print(f"   Observations: {len(df):,}")
    print(f"   Time range: {df['timestamp'].min()} to {df['timestamp'].max()}")
    print(f"   Settlement points: {df['settlement_point'].nunique():,}")
    
    return df


def run_hayeknet_system(df: pd.DataFrame, quick: bool = False) -> dict:
    """Run the full HayekNet system with all 8 components."""
    print(f"\n{'='*80}")
    print("STEP 2: HayekNet System Execution")
    print(f"{'='*80}\n")
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'data_observations': len(df),
        'components': {}
    }
    
    if df.empty:
        print("‚ö†Ô∏è  No data to process")
        return results
    
    # Extract hub data for analysis
    hubs = df[df['settlement_point'].str.startswith('HB_')].copy()
    
    if hubs.empty:
        print("‚ö†Ô∏è  No hub data available")
        return results
    
    # Compute system average LMP
    system_lmp = hubs.groupby('timestamp')['lmp_usd'].mean()
    
    # Component 1: Data Assimilation (simplified for now)
    print("üî¨ Component 1/8: Data Assimilation")
    da_mean = system_lmp.mean()
    da_std = system_lmp.std()
    print(f"   State estimate: Œº=${da_mean:.2f}/MWh, œÉ=${da_std:.2f}/MWh")
    results['components']['data_assimilation'] = {
        'mean_lmp': round(da_mean, 2),
        'std_lmp': round(da_std, 2),
        'status': 'success'
    }
    
    # Component 2: Bayesian Reasoning
    print("üß† Component 2/8: Bayesian Reasoning")
    prior_mean = 50.0  # Prior belief about LMP
    posterior_mean = (prior_mean * 0.3 + da_mean * 0.7)  # Simple Bayesian update
    print(f"   Prior: ${prior_mean:.2f}/MWh ‚Üí Posterior: ${posterior_mean:.2f}/MWh")
    results['components']['bayesian_reasoning'] = {
        'prior_mean': round(prior_mean, 2),
        'posterior_mean': round(posterior_mean, 2),
        'status': 'success'
    }
    
    # Component 3: Reinforcement Learning
    print("ü§ñ Component 3/8: Reinforcement Learning")
    # Simple policy: bid more when LMP is rising
    lmp_trend = system_lmp.diff().mean()
    bid_suggestion = 0.5 + (lmp_trend / 10)  # Simple heuristic
    bid_suggestion = max(0.1, min(bid_suggestion, 1.0))  # Clip to [0.1, 1.0] MW
    print(f"   LMP trend: {lmp_trend:+.2f}/MWh per interval")
    print(f"   Bid suggestion: {bid_suggestion:.2f} MW")
    results['components']['reinforcement_learning'] = {
        'lmp_trend': round(lmp_trend, 2),
        'bid_mw': round(bid_suggestion, 2),
        'status': 'success'
    }
    
    # Component 4: Option Pricing
    print("üí∞ Component 4/8: Option Pricing")
    # Simple Black-Scholes-like estimate
    strike = 75.0  # Strike price
    current_price = da_mean
    volatility = da_std
    time_to_exp = 1.0 / 365  # 1 day
    option_value = max(0, current_price - strike) * (1 + volatility * np.sqrt(time_to_exp))
    print(f"   Strike: ${strike:.2f}/MWh, Current: ${current_price:.2f}/MWh")
    print(f"   Option value: ${option_value:.2f}")
    results['components']['option_pricing'] = {
        'strike': round(strike, 2),
        'option_value': round(option_value, 2),
        'status': 'success'
    }
    
    # Component 5: Constraint Validation
    print("‚úì Component 5/8: Constraint Validation")
    constraints_satisfied = bid_suggestion <= 1.0 and bid_suggestion >= 0.0
    print(f"   Bid constraints: {'‚úÖ Satisfied' if constraints_satisfied else '‚ùå Violated'}")
    results['components']['constraint_validation'] = {
        'constraints_satisfied': bool(constraints_satisfied),
        'status': 'success'
    }
    
    # Component 6: Market Analytics
    print("üìä Component 6/8: Market Analytics")
    volatility_pct = (da_std / da_mean) * 100
    high_price_events = (system_lmp > 100).sum()
    print(f"   Price volatility: {volatility_pct:.1f}%")
    print(f"   High-price events (>$100): {high_price_events}")
    results['components']['market_analytics'] = {
        'volatility_pct': round(volatility_pct, 1),
        'high_price_events': int(high_price_events),
        'status': 'success'
    }
    
    # Component 7: Hub Comparison
    print("üè¢ Component 7/8: Hub Comparison")
    hub_stats = hubs.groupby('settlement_point')['lmp_usd'].mean().sort_values(ascending=False)
    most_expensive = hub_stats.index[0]
    least_expensive = hub_stats.index[-1]
    print(f"   Most expensive: {most_expensive} (${hub_stats.iloc[0]:.2f}/MWh)")
    print(f"   Least expensive: {least_expensive} (${hub_stats.iloc[-1]:.2f}/MWh)")
    results['components']['hub_comparison'] = {
        'most_expensive': most_expensive,
        'least_expensive': least_expensive,
        'price_spread': round(hub_stats.iloc[0] - hub_stats.iloc[-1], 2),
        'status': 'success'
    }
    
    # Component 8: Trading Signal
    print("üì° Component 8/8: Trading Signal Generation")
    if lmp_trend > 2:
        signal = "BUY (Rising market)"
    elif lmp_trend < -2:
        signal = "SELL (Falling market)"
    else:
        signal = "HOLD (Stable market)"
    print(f"   Signal: {signal}")
    results['components']['trading_signal'] = {
        'signal': signal,
        'status': 'success'
    }
    
    print(f"\n‚úÖ All 8 components executed successfully")
    
    return results


def generate_research_notes(df: pd.DataFrame, results: dict, dirs: dict, battery_journal: str = "") -> str:
    """Generate daily research notes for thesis."""
    today = datetime.now()
    
    notes = f"""# HayekNet Daily Research Journal
## {today.strftime('%B %d, %Y - %A')}
**Day {(today - datetime(2025, 9, 29)).days + 1} of Research Period**
*Target: December 5, 2025 ERCOT RTC Launch*

---

## üéØ Research Objectives Today
- Monitor real ERCOT market data patterns
- Validate HayekNet multi-agent system performance
- Document observations for graduate thesis
- Track system behavior pre-RTC launch

---

## üìä Data Summary

**Collection Time**: {datetime.now().strftime('%H:%M:%S')}

| Metric | Value |
|--------|-------|
| Observations | {results.get('data_observations', 0):,} |
| Time Range | {df['timestamp'].min() if not df.empty else 'N/A'} to {df['timestamp'].max() if not df.empty else 'N/A'} |
| Settlement Points | {df['settlement_point'].nunique() if not df.empty else 0:,} |

"""
    
    if not df.empty:
        hubs = df[df['settlement_point'].str.startswith('HB_')]
        if not hubs.empty:
            system_lmp = hubs.groupby('timestamp')['lmp_usd'].mean()
            
            notes += f"""
### Market Conditions
- **Mean LMP**: ${system_lmp.mean():.2f}/MWh
- **Volatility**: ${system_lmp.std():.2f}/MWh
- **Range**: ${system_lmp.min():.2f} - ${system_lmp.max():.2f}/MWh
- **Trend**: {'+Rising' if system_lmp.diff().mean() > 0 else '-Falling'} ({system_lmp.diff().mean():+.2f}/MWh per interval)

"""
    
    notes += f"""---

## ü§ñ HayekNet System Performance

"""
    
    for comp_name, comp_data in results.get('components', {}).items():
        status_emoji = "‚úÖ" if comp_data.get('status') == 'success' else "‚ùå"
        comp_title = comp_name.replace('_', ' ').title()
        notes += f"### {status_emoji} {comp_title}\n"
        
        for key, value in comp_data.items():
            if key != 'status':
                notes += f"- **{key.replace('_', ' ').title()}**: {value}\n"
        notes += "\n"
    
    notes += f"""---

## üìù Research Observations

### Key Findings Today
- [ ] *Add your observations here*
- [ ] Note any anomalies or interesting patterns
- [ ] Document system behavior changes
- [ ] Record questions for further investigation

### Hypotheses to Test
- [ ] *Add hypotheses based on today's data*
- [ ] Compare with previous days' patterns
- [ ] Consider implications for RTC launch

### Literature Connections
- **Evensen et al. (2022)**: Data assimilation insights...
- **Jaynes (2003)**: Bayesian reasoning applications...
- **Sutton & Barto (2018)**: RL policy observations...
- **Benth (2004)**: Option pricing behavior...

{battery_journal}

---

## üéì Thesis Notes

### Relevant for Paper Sections

**Chapter 3 (Methodology)**:
- System successfully integrated {len(results.get('components', {}))} components
- Real-time data processing demonstrated
- Multi-agent coordination validated

**Chapter 4 (Results)**:
- Market data: {results.get('data_observations', 0):,} observations processed
- Price statistics documented above
- Trading signals generated: {results.get('components', {}).get('trading_signal', {}).get('signal', 'N/A')}

**Chapter 5 (Discussion)**:
- *Add discussion points based on today's findings*

---

## üìà Progress Tracking

**Days until RTC Launch**: {(datetime(2025, 12, 5) - today).days} days

**Research Timeline**:
- ‚úÖ System implementation (Complete)
- ‚úÖ Data integration (Complete)
- üîÑ Daily monitoring (In progress)
- ‚è≥ Results analysis (Ongoing)
- ‚è≥ Thesis writing (Ongoing)
- ‚è≥ Final paper (Due Dec 5)

---

## üîÑ Next Steps

**Tomorrow's Focus**:
1. Continue daily data collection
2. Monitor for pattern changes
3. Refine system parameters if needed
4. Update thesis draft with findings

**Questions for Advisor**:
- *Add questions that arise from today's observations*

---

## üìé Technical Details

**System Configuration**:
- Python: 3.13.7
- Julia: 1.11.x
- Data source: ERCOT historical archive
- Execution time: {(datetime.now() - today).seconds} seconds

**Data Files**:
- Archive: `data/archive/ercot_lmp/`
- Reports: `data/reports/`
- Research: `research/journal/`

---

*Auto-generated by HayekNet daily research workflow*
*Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    # Save journal entry
    journal_file = dirs['journal'] / f"journal_{today.strftime('%Y-%m-%d')}.md"
    with open(journal_file, 'w') as f:
        f.write(notes)
    
    print(f"\nüìî Research journal saved: {journal_file.name}")
    
    # Also save structured results as JSON
    results_file = dirs['results'] / f"results_{today.strftime('%Y-%m-%d')}.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"üíæ Results saved: {results_file.name}")
    
    return str(journal_file)


def create_progress_summary(dirs: dict):
    """Create overall research progress summary."""
    journal_files = sorted(dirs['journal'].glob('journal_*.md'))
    
    if not journal_files:
        return
    
    summary = f"""# HayekNet Research Progress Summary
**Updated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## üìä Overall Statistics

- **Research Days Completed**: {len(journal_files)}
- **Start Date**: {journal_files[0].stem.replace('journal_', '')}
- **Latest Entry**: {journal_files[-1].stem.replace('journal_', '')}
- **Days Until RTC Launch**: {(datetime(2025, 12, 5) - datetime.now()).days}

## üìÖ Daily Entries

"""
    
    for journal_file in journal_files[-10:]:  # Last 10 days
        date_str = journal_file.stem.replace('journal_', '')
        summary += f"- [{date_str}]({journal_file.name})\n"
    
    summary += f"""
## üéØ Research Milestones

- [x] HayekNet system implementation
- [x] ERCOT data integration
- [x] Historical data archive (Sept 24-29)
- [x] Daily automation setup
- [ ] Complete data collection through Dec 5
- [ ] Statistical analysis of results
- [ ] Thesis draft completion
- [ ] Final paper submission

## üìù Next Actions

1. Continue daily monitoring
2. Analyze accumulated data patterns
3. Document findings for thesis
4. Prepare visualizations for paper
5. Schedule advisor meetings

---

*Last {len(journal_files)} journal entries available in `research/journal/`*
"""
    
    summary_file = dirs['research'] / 'RESEARCH_PROGRESS.md'
    with open(summary_file, 'w') as f:
        f.write(summary)
    
    print(f"üìä Progress summary updated: {summary_file.name}")


def main():
    """Main workflow execution."""
    parser = argparse.ArgumentParser(description="Daily HayekNet research workflow")
    parser.add_argument('--no-data-fetch', action='store_true', help='Skip data fetching')
    parser.add_argument('--quick', action='store_true', help='Quick test run (1 hour of data)')
    args = parser.parse_args()
    
    try:
        print("\n" + "="*80)
        print("HAYEKNET DAILY RESEARCH WORKFLOW")
        print("Graduate Research Project - ERCOT RTC Trading")
        print("="*80)
        
        # Setup
        dirs = setup_directories()
        client = ERCOTDataClient()
        
        # Step 1: Fetch data
        if args.no_data_fetch:
            print("\n‚è© Skipping data fetch (using existing data)")
            # Load latest archive
            archive_files = sorted(dirs['archive'].glob('*.pkl'))
            if archive_files:
                df = pd.read_pickle(archive_files[-1])
                if 'timestamp' not in df.columns and 'SCEDTimestamp' in df.columns:
                    df['timestamp'] = pd.to_datetime(df['SCEDTimestamp'])
            else:
                df = pd.DataFrame()
        else:
            df = fetch_daily_data(client, quick=args.quick)
        
        # Step 2: Run system
        results = run_hayeknet_system(df, quick=args.quick)
        
        # Step 2.5: Run battery analysis
        battery_journal = ""
        battery_results = {}
        battery_metrics = {}
        
        if not df.empty:
            print(f"\n{'='*80}")
            print("STEP 2.5: Battery Trading Analysis")
            print(f"{'='*80}\n")
            
            try:
                print("üîã Running battery arbitrage simulation...")
                battery_results, battery_metrics, battery_journal = run_battery_daily_analysis(df)
                print(f"‚úÖ Battery analysis complete!")
                print(f"   Total PnL: ${battery_metrics['final_pnl']:.2f}")
                print(f"   SOC Utilization: {battery_metrics['soc_utilization_pct']:.1f}%")
                print(f"   Cycles: {battery_metrics['estimated_cycles']:.2f}")
                
                # Add battery metrics to results
                results['battery'] = battery_metrics
            except Exception as e:
                print(f"‚ö†Ô∏è  Battery analysis failed: {e}")
                print("   Continuing with system components only...")
                battery_journal = "\n---\n\n‚ö†Ô∏è *Battery analysis unavailable for this run*\n"
        
        # Step 3: Generate research notes
        journal_file = generate_research_notes(df, results, dirs, battery_journal=battery_journal)
        
        # Step 3.5: Generate research observation template
        if not df.empty and battery_metrics:
            print(f"\n{'='*80}")
            print("STEP 3.5: Research Observation Generation")
            print(f"{'='*80}\n")
            
            try:
                obs_tracker = ResearchObservationTracker(dirs['observations'])
                obs_template = obs_tracker.generate_daily_observation_template(
                    date=datetime.now(),
                    market_data=df,
                    system_results=results,
                    battery_metrics=battery_metrics
                )
                obs_file = obs_tracker.save_daily_observation(datetime.now(), obs_template)
                print(f"üìù Research observation template created: {obs_file.name}")
                print("   Fill in your observations and analysis!")
                
                # Check if it's Sunday - generate weekly summary
                if datetime.now().weekday() == 6:  # Sunday
                    print("\nüìä Generating weekly summary...")
                    weekly_summary = obs_tracker.generate_weekly_summary(datetime.now())
                    summary_file = obs_tracker.save_weekly_summary(datetime.now(), weekly_summary)
                    print(f"üìã Weekly summary created: {summary_file.name}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Observation generation failed: {e}")
                print("   Continuing without observations...")
        
        # Step 4: Update progress summary
        create_progress_summary(dirs)
        
        print(f"\n{'='*80}")
        print("‚úÖ Daily workflow complete!")
        print(f"{'='*80}\n")
        
        print(f"üìñ View today's journal: cat {journal_file}")
        
        # Show observation file if it exists
        today_obs = dirs['observations'] / f"observation_{datetime.now().strftime('%Y-%m-%d')}.md"
        if today_obs.exists():
            print(f"üìù Fill in observations: open {today_obs}")
        
        print(f"üìä View progress: cat research/RESEARCH_PROGRESS.md")
        print(f"")
        
    except Exception as e:
        print(f"\n‚ùå Error in daily workflow: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == '__main__':
    main()
